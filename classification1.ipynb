{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "219b1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") #breaks down text into individual units of meaning\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68f147ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a training dataset and a validation \n",
    "# dataset from our \"aclImdb/train\" directory with a 80/20 split.\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "train = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'C:\\\\Users\\\\Viktorija\\\\Desktop\\\\JT\\\\SV2\\\\separatedText', batch_size=4000, validation_split=0.2, \n",
    "    subset='training', seed=123)\n",
    "test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'C:\\\\Users\\\\Viktorija\\\\Desktop\\\\JT\\\\SV2\\\\separatedText', batch_size=4000, validation_split=0.2, \n",
    "    subset='validation', seed=123)\n",
    "\n",
    "#.numpy() -> converts an array-like object into a numpy array\n",
    "\n",
    "for i in train.take(1):\n",
    "    train_feat = i[0].numpy() #array of the tweets\n",
    "    train_lab = i[1].numpy() #array of their ratings\n",
    "\n",
    "#DataFrame contains labeled axes (rows and columns). Can be thought of as a dict-like container.\n",
    "#puts the data in the format: nr_of_tweet tweet sentiment_value\n",
    "train = pd.DataFrame([train_feat, train_lab]).T\n",
    "\n",
    "#in this case, data is the tweet, and label is the sentiment value\n",
    "train.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "train['DATA_COLUMN'] = train['DATA_COLUMN'].str.decode(\"utf-8\")\n",
    "#train.head() #returns the first N (N=5) entries in the train DataFrame\n",
    "\n",
    "for j in test.take(1):\n",
    "    test_feat = j[0].numpy()\n",
    "    test_lab = j[1].numpy()\n",
    "\n",
    "test = pd.DataFrame([test_feat, test_lab]).T\n",
    "test.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "test['DATA_COLUMN'] = test['DATA_COLUMN'].str.decode(\"utf-8\")\n",
    "#test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c02523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "    \n",
    "    #train and test are pandas DataFrames, the other two are strings\n",
    "    \n",
    "    #.apply -> Apply the function x on the columns (axis = 1) of train. \n",
    "    #Converts the train table into a collection of InputExamples for BERT processing.\n",
    "    #text_a is the tweet, label is the sentiment value.\n",
    "    train_InputExamples = train.apply(lambda x: InputExample(guid=None, text_a = x[DATA_COLUMN], text_b = None, label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, text_a = x[DATA_COLUMN], text_b = None, label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "    return train_InputExamples, validation_InputExamples\n",
    "\n",
    "    #train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, 'DATA_COLUMN', 'LABEL_COLUMN')\n",
    "    \n",
    "    \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    \n",
    "    #examples is a list of InputExamples\n",
    "    \n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    #for each InputExample\n",
    "    for e in examples:\n",
    "        \n",
    "        #encode_plus encodes a text input as a set of numerical inputs that can be used as input to a model.\n",
    "        #text_a is the input text to be encoded\n",
    "        #special tokens are used to indicate the start and the end of the encoded input text\n",
    "        #max_length is the maximum length of the encoded input\n",
    "        #token_type_ids should be returned along with the encoded input\n",
    "        #attention_mask is a binary mask that indicates which input tokens should be attended to by the model, and which ones should be ignored. An attention mask is needed, because most inputs are padded and the added zeroes (usually at the end) shouldn't be attended to by the model.\n",
    "        #the encoded input should be padded with special padding tokens to ensure that all inputs have the same length\n",
    "        #input should be truncated if it exceeds max_length\n",
    "        \n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "        \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'DATA_COLUMN'\n",
    "LABEL_COLUMN = 'LABEL_COLUMN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b14905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the train and test tables into collections of InputExamples for BERT processing.\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "#Convert the InputExamples into tf.data.Datasets\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "#model.summary()\n",
    "model.fit(train_data, epochs=2, validation_data=validation_data, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
